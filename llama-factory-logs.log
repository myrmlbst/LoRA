[INFO|2025-02-21 13:39:55] llamafactory.hparams.parser:384 >> Process rank: 0, device: mps, n_gpu: 1, distributed training: False, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:697] 2025-02-21 13:39:55,377 >> loading configuration file Qwen/Qwen2.5-1.5B-Instruct/config.json
[INFO|configuration_utils.py:771] 2025-02-21 13:39:55,378 >> Model config Qwen2Config {
  "_name_or_path": "Qwen/Qwen2.5-1.5B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.49.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2048] 2025-02-21 13:39:55,380 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2048] 2025-02-21 13:39:55,381 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2048] 2025-02-21 13:39:55,381 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2048] 2025-02-21 13:39:55,381 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2048] 2025-02-21 13:39:55,381 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2048] 2025-02-21 13:39:55,381 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2048] 2025-02-21 13:39:55,381 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2313] 2025-02-21 13:39:55,557 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:697] 2025-02-21 13:39:55,559 >> loading configuration file Qwen/Qwen2.5-1.5B-Instruct/config.json
[INFO|configuration_utils.py:771] 2025-02-21 13:39:55,559 >> Model config Qwen2Config {
  "_name_or_path": "Qwen/Qwen2.5-1.5B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.49.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2048] 2025-02-21 13:39:55,561 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2048] 2025-02-21 13:39:55,561 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2048] 2025-02-21 13:39:55,561 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2048] 2025-02-21 13:39:55,561 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2048] 2025-02-21 13:39:55,561 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2048] 2025-02-21 13:39:55,561 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2048] 2025-02-21 13:39:55,561 >> loading file chat_template.jinja
[INFO|tokenization_utils_base.py:2313] 2025-02-21 13:39:55,731 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2025-02-21 13:39:55] llamafactory.data.template:157 >> Add <|im_end|> to stop words.
[INFO|2025-02-21 13:39:55] llamafactory.data.loader:157 >> Loading dataset labels.json...
Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.
Generating train split: 100 examples [00:00, 10640.58 examples/s]
Converting format of dataset (num_proc=16): 100%|███████████████████████████████████████████| 100/100 [00:00<00:00, 657.74 examples/s]
Running tokenizer on dataset (num_proc=16): 100%|████████████████████████████████████████████| 100/100 [00:01<00:00, 55.27 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 458, 6203, 304, 53526, 328, 18723, 62547, 22507, 22619, 8085, 320, 39779, 40, 4292, 63316, 821, 2924, 25, 5036, 11, 2551, 14230, 11, 14230, 11, 4540, 5109, 11, 323, 3590, 4763, 5109, 624, 7771, 16538, 374, 311, 23643, 264, 1196, 1943, 11, 323, 470, 264, 42148, 2319, 448, 678, 20429, 40, 821, 12575, 448, 264, 14879, 1248, 151645, 198, 151644, 872, 198, 334, 31382, 135062, 66963, 128707, 126381, 127837, 0, 52157, 124124, 127837, 128769, 123920, 41593, 124173, 126196, 74315, 13325, 124325, 125503, 98719, 13, 127046, 126298, 124034, 23364, 129520, 125492, 128438, 128332, 2303, 334, 20064, 124226, 66963, 128707, 126381, 127837, 68785, 125998, 125709, 124130, 25871, 77273, 68238, 136176, 14558, 124766, 126157, 128252, 127072, 127386, 13, 2303, 334, 31382, 135062, 66963, 124476, 135910, 68785, 59842, 124226, 13, 59842, 69682, 124655, 59842, 126624, 127837, 27846, 124223, 126606, 125492, 13, 129859, 132148, 39434, 39697, 124623, 79820, 135208, 31073, 140286, 128332, 2303, 334, 20064, 124226, 66963, 12961, 127684, 59842, 124226, 131118, 13, 2303, 334, 31382, 135062, 66963, 52157, 124124, 127837, 129361, 68785, 59842, 124226, 131118, 13, 126198, 128321, 126195, 126277, 85153, 129344, 125492, 128332, 2303, 334, 20064, 124226, 66963, 128962, 124218, 77273, 52157, 35038, 23224, 124080, 142489, 68785, 130918, 13, 2303, 334, 31382, 135062, 66963, 52157, 124124, 127837, 129361, 128248, 132733, 13, 129859, 132148, 39434, 39697, 124623, 79820, 129920, 125821, 137209, 31073, 128332, 2303, 334, 20064, 124226, 66963, 50243, 124423, 68785, 131273, 14558, 128321, 220, 15, 20, 20, 16, 17, 18, 19, 20, 21, 22, 13, 2303, 334, 31382, 135062, 66963, 52157, 124124, 127837, 68785, 124766, 126552, 127837, 68785, 59842, 69682, 126157, 128252, 131273, 125100, 126395, 132260, 130245, 129834, 125006, 136128, 63237, 68238, 136176, 31073, 13, 2303, 334, 20064, 124226, 66963, 124476, 135910, 68785, 131273, 14558, 128321, 220, 16, 17, 18, 19, 20, 21, 22, 23, 24, 15, 13, 2303, 334, 31382, 135062, 66963, 52157, 124124, 127837, 129361, 68785, 128510, 94957, 134860, 63237, 68238, 136176, 31073, 13, 127046, 126298, 124034, 23364, 129520, 125492, 77273, 130260, 143332, 128332, 2303, 334, 20064, 124226, 66963, 63415, 131377, 125709, 124130, 25871, 77273, 130243, 127039, 124220, 35244, 72804, 128252, 68238, 136176, 14558, 13, 2303, 334, 31382, 135062, 66963, 59842, 125592, 126606, 31073, 77273, 128349, 13, 129859, 132148, 85153, 35244, 124422, 14558, 27846, 126332, 124865, 130648, 69682, 128261, 39434, 129198, 129361, 128332, 2303, 334, 20064, 124226, 66963, 39434, 129198, 125502, 139268, 39434, 126311, 330, 124130, 124325, 125637, 58656, 128478, 92072, 124668, 124290, 3263, 2303, 334, 31382, 135062, 66963, 68238, 132138, 127837, 68785, 59842, 69682, 125856, 130394, 126967, 39434, 125090, 123860, 136367, 125637, 58656, 129361, 13, 129859, 132148, 39434, 69682, 31073, 123980, 27846, 125325, 31073, 134155, 128332, 2303, 334, 20064, 124226, 66963, 50243, 124423, 68785, 27846, 123832, 79820, 134155, 128321, 28047, 1466, 13, 1466, 2061, 35487, 905, 13, 2303, 334, 31382, 135062, 66963, 52157, 124124, 127837, 68785, 77703, 126406, 130394, 126332, 31382, 53710, 70604, 43635, 136950, 39434, 125090, 123860, 136367, 125637, 58656, 128252, 27846, 125325, 31073, 134155, 13, 129859, 128954, 124793, 131113, 129197, 126298, 124034, 23364, 129520, 125492, 81768, 128332, 2303, 334, 20064, 124226, 66963, 128259, 68785, 52157, 124124, 127837, 129361, 128248, 23364, 129520, 125492, 13, 2303, 334, 31382, 135062, 66963, 128248, 124269, 126381, 134274, 124511, 0, 128754, 12961, 29825, 124425, 14293, 128252, 124793, 131113, 77273, 138471, 68785, 132225, 39434, 141707, 77273, 135203, 126196, 124104, 13, 128587, 31073, 59842, 126624, 0, 151645, 198, 151644, 77091, 198, 334, 31382, 135062, 66963, 128707, 126381, 127837, 0, 52157, 124124, 127837, 128769, 123920, 41593, 124173, 126196, 74315, 13325, 124325, 125503, 98719, 13, 127046, 126298, 124034, 23364, 129520, 125492, 128438, 128332, 2303, 334, 20064, 124226, 66963, 128707, 126381, 127837, 68785, 125998, 125709, 124130, 25871, 77273, 68238, 136176, 14558, 124766, 126157, 128252, 127072, 127386, 13, 2303, 334, 31382, 135062, 66963, 124476, 135910, 68785, 59842, 124226, 13, 59842, 69682, 124655, 59842, 126624, 127837, 27846, 124223, 126606, 125492, 13, 129859, 132148, 39434, 39697, 124623, 79820, 135208, 31073, 140286, 128332, 2303, 334, 20064, 124226, 66963, 12961, 127684, 30704, 41423, 13, 2303, 334, 31382, 135062, 66963, 52157, 124124, 127837, 129361, 68785, 30704, 41423, 13, 126198, 128321, 126195, 126277, 85153, 129344, 125492, 128332, 2303, 334, 20064, 124226, 66963, 128962, 124218, 77273, 41423, 30704, 68785, 41423, 13, 2303, 334, 31382, 135062, 66963, 52157, 124124, 127837, 129361, 128248, 132733, 13, 129859, 132148, 39434, 39697, 124623, 79820, 129920, 125821, 137209, 31073, 128332, 2303, 334, 20064, 124226, 66963, 50243, 124423, 68785, 131273, 14558, 128321, 34145, 334, 13, 2303, 334, 31382, 135062, 66963, 52157, 124124, 127837, 68785, 124766, 126552, 127837, 68785, 59842, 69682, 126157, 128252, 131273, 125100, 126395, 132260, 130245, 129834, 125006, 136128, 63237, 68238, 136176, 31073, 13, 2303, 334, 20064, 124226, 66963, 124476, 135910, 68785, 131273, 14558, 128321, 34145, 334, 13, 2303, 334, 31382, 135062, 66963, 52157, 124124, 127837, 129361, 68785, 128510, 94957, 134860, 63237, 68238, 136176, 31073, 13, 127046, 126298, 124034, 23364, 129520, 125492, 77273, 130260, 143332, 128332, 2303, 334, 20064, 124226, 66963, 63415, 131377, 125709, 124130, 25871, 77273, 130243, 127039, 124220, 35244, 72804, 128252, 68238, 136176, 14558, 13, 2303, 334, 31382, 135062, 66963, 59842, 125592, 126606, 31073, 77273, 128349, 13, 129859, 132148, 85153, 35244, 124422, 14558, 27846, 126332, 124865, 130648, 69682, 128261, 39434, 129198, 129361, 128332, 2303, 334, 20064, 124226, 66963, 39434, 129198, 125502, 139268, 39434, 126311, 330, 124130, 124325, 125637, 58656, 128478, 92072, 124668, 124290, 3263, 2303, 334, 31382, 135062, 66963, 68238, 132138, 127837, 68785, 59842, 69682, 125856, 130394, 126967, 39434, 125090, 123860, 136367, 125637, 58656, 129361, 13, 129859, 132148, 39434, 69682, 31073, 123980, 27846, 125325, 31073, 134155, 128332, 2303, 334, 20064, 124226, 66963, 50243, 124423, 68785, 27846, 123832, 79820, 134155, 128321, 34145, 31, 18275, 13, 2303, 334, 31382, 135062, 66963, 52157, 124124, 127837, 68785, 77703, 126406, 130394, 126332, 31382, 53710, 70604, 43635, 136950, 39434, 125090, 123860, 136367, 125637, 58656, 128252, 27846, 125325, 31073, 134155, 13, 129859, 128954, 124793, 131113, 129197, 126298, 124034, 23364, 129520, 125492, 81768, 128332, 2303, 334, 20064, 124226, 66963, 128259, 68785, 52157, 124124, 127837, 129361, 128248, 23364, 129520, 125492, 13, 2303, 334, 31382, 135062, 66963, 128248, 124269, 126381, 134274, 124511, 0, 128754, 12961, 29825, 124425, 14293, 128252, 124793, 131113, 77273, 138471, 68785, 132225, 39434, 141707, 77273, 135203, 126196, 124104, 13, 128587, 31073, 59842, 126624, 0, 151645, 198]
inputs:
<|im_start|>system
You are an expert in detecting Sensitive Personally Identifiable Information (SPII).
Sensitive data include: names, email addresses, addresses, phone numbers, and social security numbers.
Your objective is to analyze a user message, and return a masked version with all SPII data replaced with a '*'
<|im_end|>
<|im_start|>user
**الموظف:** مرحبًا! شكرًا لتواصلك مع خدمة العملاء. كيف يمكنني مساعدتك اليوم؟  
**سارة:** مرحبًا، لدي مشكلة في حسابي وأحتاج إلى المساعدة.  
**الموظف:** بالطبع، سارة. سأكون سعيدًا بمساعدتك. هل يمكنك تزويدي باسمك الكامل؟  
**سارة:** اسمي سارة أحمد.  
**الموظف:** شكرًا لك، سارة أحمد. ما هو عنوان إقامتك؟  
**سارة:** أسكن في شارع النخيل، الرياض.  
**الموظف:** شكرًا لك على المعلومات. هل يمكنك تزويدي برقم هاتفك؟  
**سارة:** نعم، رقمي هو 0551234567.  
**الموظف:** شكرًا، وأخيرًا، سأحتاج إلى رقم الهوية الوطنية الخاصة بك للتحقق من حسابك.  
**سارة:** بالطبع، رقمي هو 1234567890.  
**الموظف:** شكرًا لك، تم التحقق من حسابك. كيف يمكنني مساعدتك في حل المشكلة؟  
**سارة:** أواجه مشكلة في تسجيل الدخول إلى حسابي.  
**الموظف:** سأساعدك في ذلك. هل يمكنك إخباري برسالة الخطأ التي تظهر لك؟  
**سارة:** تظهر لي رسالة تقول "كلمة المرور غير صحيحة".  
**الموظف:** حسنًا، سأقوم بإعادة تعيين كلمة المرور لك. هل يمكنك تأكيد بريدك الإلكتروني؟  
**سارة:** نعم، بريدي الإلكتروني هو sarah.ahmed@example.com.  
**الموظف:** شكرًا، قمت بإرسال رابط إعادة تعيين كلمة المرور إلى بريدك الإلكتروني. هل هناك أي شيء آخر يمكنني مساعدتك به؟  
**سارة:** لا، شكرًا لك على مساعدتك.  
**الموظف:** على الرحب والسعة! إذا احتجت إلى أي شيء في المستقبل، فلا تتردد في التواصل معنا. يومك سعيد!<|im_end|>
<|im_start|>assistant
**الموظف:** مرحبًا! شكرًا لتواصلك مع خدمة العملاء. كيف يمكنني مساعدتك اليوم؟  
**سارة:** مرحبًا، لدي مشكلة في حسابي وأحتاج إلى المساعدة.  
**الموظف:** بالطبع، سارة. سأكون سعيدًا بمساعدتك. هل يمكنك تزويدي باسمك الكامل؟  
**سارة:** اسمي **** *****.  
**الموظف:** شكرًا لك، **** *****. ما هو عنوان إقامتك؟  
**سارة:** أسكن في ***** ****، *****.  
**الموظف:** شكرًا لك على المعلومات. هل يمكنك تزويدي برقم هاتفك؟  
**سارة:** نعم، رقمي هو **********.  
**الموظف:** شكرًا، وأخيرًا، سأحتاج إلى رقم الهوية الوطنية الخاصة بك للتحقق من حسابك.  
**سارة:** بالطبع، رقمي هو **********.  
**الموظف:** شكرًا لك، تم التحقق من حسابك. كيف يمكنني مساعدتك في حل المشكلة؟  
**سارة:** أواجه مشكلة في تسجيل الدخول إلى حسابي.  
**الموظف:** سأساعدك في ذلك. هل يمكنك إخباري برسالة الخطأ التي تظهر لك؟  
**سارة:** تظهر لي رسالة تقول "كلمة المرور غير صحيحة".  
**الموظف:** حسنًا، سأقوم بإعادة تعيين كلمة المرور لك. هل يمكنك تأكيد بريدك الإلكتروني؟  
**سارة:** نعم، بريدي الإلكتروني هو ********@*****.  
**الموظف:** شكرًا، قمت بإرسال رابط إعادة تعيين كلمة المرور إلى بريدك الإلكتروني. هل هناك أي شيء آخر يمكنني مساعدتك به؟  
**سارة:** لا، شكرًا لك على مساعدتك.  
**الموظف:** على الرحب والسعة! إذا احتجت إلى أي شيء في المستقبل، فلا تتردد في التواصل معنا. يومك سعيد!<|im_end|>

label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 334, 31382, 135062, 66963, 128707, 126381, 127837, 0, 52157, 124124, 127837, 128769, 123920, 41593, 124173, 126196, 74315, 13325, 124325, 125503, 98719, 13, 127046, 126298, 124034, 23364, 129520, 125492, 128438, 128332, 2303, 334, 20064, 124226, 66963, 128707, 126381, 127837, 68785, 125998, 125709, 124130, 25871, 77273, 68238, 136176, 14558, 124766, 126157, 128252, 127072, 127386, 13, 2303, 334, 31382, 135062, 66963, 124476, 135910, 68785, 59842, 124226, 13, 59842, 69682, 124655, 59842, 126624, 127837, 27846, 124223, 126606, 125492, 13, 129859, 132148, 39434, 39697, 124623, 79820, 135208, 31073, 140286, 128332, 2303, 334, 20064, 124226, 66963, 12961, 127684, 30704, 41423, 13, 2303, 334, 31382, 135062, 66963, 52157, 124124, 127837, 129361, 68785, 30704, 41423, 13, 126198, 128321, 126195, 126277, 85153, 129344, 125492, 128332, 2303, 334, 20064, 124226, 66963, 128962, 124218, 77273, 41423, 30704, 68785, 41423, 13, 2303, 334, 31382, 135062, 66963, 52157, 124124, 127837, 129361, 128248, 132733, 13, 129859, 132148, 39434, 39697, 124623, 79820, 129920, 125821, 137209, 31073, 128332, 2303, 334, 20064, 124226, 66963, 50243, 124423, 68785, 131273, 14558, 128321, 34145, 334, 13, 2303, 334, 31382, 135062, 66963, 52157, 124124, 127837, 68785, 124766, 126552, 127837, 68785, 59842, 69682, 126157, 128252, 131273, 125100, 126395, 132260, 130245, 129834, 125006, 136128, 63237, 68238, 136176, 31073, 13, 2303, 334, 20064, 124226, 66963, 124476, 135910, 68785, 131273, 14558, 128321, 34145, 334, 13, 2303, 334, 31382, 135062, 66963, 52157, 124124, 127837, 129361, 68785, 128510, 94957, 134860, 63237, 68238, 136176, 31073, 13, 127046, 126298, 124034, 23364, 129520, 125492, 77273, 130260, 143332, 128332, 2303, 334, 20064, 124226, 66963, 63415, 131377, 125709, 124130, 25871, 77273, 130243, 127039, 124220, 35244, 72804, 128252, 68238, 136176, 14558, 13, 2303, 334, 31382, 135062, 66963, 59842, 125592, 126606, 31073, 77273, 128349, 13, 129859, 132148, 85153, 35244, 124422, 14558, 27846, 126332, 124865, 130648, 69682, 128261, 39434, 129198, 129361, 128332, 2303, 334, 20064, 124226, 66963, 39434, 129198, 125502, 139268, 39434, 126311, 330, 124130, 124325, 125637, 58656, 128478, 92072, 124668, 124290, 3263, 2303, 334, 31382, 135062, 66963, 68238, 132138, 127837, 68785, 59842, 69682, 125856, 130394, 126967, 39434, 125090, 123860, 136367, 125637, 58656, 129361, 13, 129859, 132148, 39434, 69682, 31073, 123980, 27846, 125325, 31073, 134155, 128332, 2303, 334, 20064, 124226, 66963, 50243, 124423, 68785, 27846, 123832, 79820, 134155, 128321, 34145, 31, 18275, 13, 2303, 334, 31382, 135062, 66963, 52157, 124124, 127837, 68785, 77703, 126406, 130394, 126332, 31382, 53710, 70604, 43635, 136950, 39434, 125090, 123860, 136367, 125637, 58656, 128252, 27846, 125325, 31073, 134155, 13, 129859, 128954, 124793, 131113, 129197, 126298, 124034, 23364, 129520, 125492, 81768, 128332, 2303, 334, 20064, 124226, 66963, 128259, 68785, 52157, 124124, 127837, 129361, 128248, 23364, 129520, 125492, 13, 2303, 334, 31382, 135062, 66963, 128248, 124269, 126381, 134274, 124511, 0, 128754, 12961, 29825, 124425, 14293, 128252, 124793, 131113, 77273, 138471, 68785, 132225, 39434, 141707, 77273, 135203, 126196, 124104, 13, 128587, 31073, 59842, 126624, 0, 151645, 198]
labels:
**الموظف:** مرحبًا! شكرًا لتواصلك مع خدمة العملاء. كيف يمكنني مساعدتك اليوم؟  
**سارة:** مرحبًا، لدي مشكلة في حسابي وأحتاج إلى المساعدة.  
**الموظف:** بالطبع، سارة. سأكون سعيدًا بمساعدتك. هل يمكنك تزويدي باسمك الكامل؟  
**سارة:** اسمي **** *****.  
**الموظف:** شكرًا لك، **** *****. ما هو عنوان إقامتك؟  
**سارة:** أسكن في ***** ****، *****.  
**الموظف:** شكرًا لك على المعلومات. هل يمكنك تزويدي برقم هاتفك؟  
**سارة:** نعم، رقمي هو **********.  
**الموظف:** شكرًا، وأخيرًا، سأحتاج إلى رقم الهوية الوطنية الخاصة بك للتحقق من حسابك.  
**سارة:** بالطبع، رقمي هو **********.  
**الموظف:** شكرًا لك، تم التحقق من حسابك. كيف يمكنني مساعدتك في حل المشكلة؟  
**سارة:** أواجه مشكلة في تسجيل الدخول إلى حسابي.  
**الموظف:** سأساعدك في ذلك. هل يمكنك إخباري برسالة الخطأ التي تظهر لك؟  
**سارة:** تظهر لي رسالة تقول "كلمة المرور غير صحيحة".  
**الموظف:** حسنًا، سأقوم بإعادة تعيين كلمة المرور لك. هل يمكنك تأكيد بريدك الإلكتروني؟  
**سارة:** نعم، بريدي الإلكتروني هو ********@*****.  
**الموظف:** شكرًا، قمت بإرسال رابط إعادة تعيين كلمة المرور إلى بريدك الإلكتروني. هل هناك أي شيء آخر يمكنني مساعدتك به؟  
**سارة:** لا، شكرًا لك على مساعدتك.  
**الموظف:** على الرحب والسعة! إذا احتجت إلى أي شيء في المستقبل، فلا تتردد في التواصل معنا. يومك سعيد!<|im_end|>

[INFO|configuration_utils.py:697] 2025-02-21 13:39:58,232 >> loading configuration file Qwen/Qwen2.5-1.5B-Instruct/config.json
[INFO|configuration_utils.py:771] 2025-02-21 13:39:58,233 >> Model config Qwen2Config {
  "_name_or_path": "Qwen/Qwen2.5-1.5B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.49.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|modeling_utils.py:3979] 2025-02-21 13:39:58,767 >> loading weights file Qwen/Qwen2.5-1.5B-Instruct/model.safetensors
[INFO|modeling_utils.py:1633] 2025-02-21 13:39:58,814 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1140] 2025-02-21 13:39:58,816 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

[WARNING|logging.py:329] 2025-02-21 13:39:58,822 >> Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.
[INFO|modeling_utils.py:4970] 2025-02-21 13:40:01,695 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4978] 2025-02-21 13:40:01,695 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-1.5B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1093] 2025-02-21 13:40:01,702 >> loading configuration file Qwen/Qwen2.5-1.5B-Instruct/generation_config.json
[INFO|configuration_utils.py:1140] 2025-02-21 13:40:01,702 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.1,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

[INFO|2025-02-21 13:40:01] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.
[INFO|2025-02-21 13:40:01] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.
[INFO|2025-02-21 13:40:01] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.
[INFO|2025-02-21 13:40:01] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA
[INFO|2025-02-21 13:40:01] llamafactory.model.model_utils.misc:157 >> Found linear modules: up_proj,down_proj,q_proj,v_proj,gate_proj,k_proj,o_proj
[INFO|2025-02-21 13:40:02] llamafactory.model.loader:157 >> trainable params: 9,232,384 || all params: 1,552,946,688 || trainable%: 0.5945
[INFO|trainer.py:514] 2025-02-21 13:40:02,370 >> You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.
[INFO|trainer.py:746] 2025-02-21 13:40:02,372 >> Using auto half precision backend
[WARNING|trainer.py:781] 2025-02-21 13:40:02,373 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.
[INFO|trainer.py:2405] 2025-02-21 13:40:02,498 >> ***** Running training *****
[INFO|trainer.py:2406] 2025-02-21 13:40:02,498 >>   Num examples = 90
[INFO|trainer.py:2407] 2025-02-21 13:40:02,498 >>   Num Epochs = 3
[INFO|trainer.py:2408] 2025-02-21 13:40:02,498 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2411] 2025-02-21 13:40:02,498 >>   Total train batch size (w. parallel, distributed & accumulation) = 4
[INFO|trainer.py:2412] 2025-02-21 13:40:02,498 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2413] 2025-02-21 13:40:02,498 >>   Total optimization steps = 66
[INFO|trainer.py:2414] 2025-02-21 13:40:02,500 >>   Number of trainable parameters = 9,232,384
  0%|                                                                                                          | 0/66 [00:00<?, ?it/s]./LLaMA-Factory/.venv/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.
  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]
{'loss': 0.1109, 'grad_norm': 0.2166227251291275, 'learning_rate': 9.936341773606723e-05, 'epoch': 0.44}                              
{'loss': 0.0623, 'grad_norm': 0.23902013897895813, 'learning_rate': 8.849169917149531e-05, 'epoch': 0.89}                             
{'loss': 0.0415, 'grad_norm': 0.18312686681747437, 'learning_rate': 6.696194330590151e-05, 'epoch': 1.31}                             
{'loss': 0.043, 'grad_norm': 0.3239552974700928, 'learning_rate': 4.0735563795644294e-05, 'epoch': 1.76}                              
 70%|███████████████████████████████████████████████████████████████████▌                             | 46/66 [12:36<05:27, 16.39s/it]
{'loss': 0.0343, 'grad_norm': 0.13738977909088135, 'learning_rate': 1.7074431046748075e-05, 'epoch': 2.18}                            
{'loss': 0.0332, 'grad_norm': 0.1185305193066597, 'learning_rate': 2.530119576580936e-06, 'epoch': 2.62}                              
100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 66/66 [18:16<00:00, 15.91s/it][INFO|trainer.py:3942] 2025-02-21 13:58:19,424 >> Saving model checkpoint to checkpoints/checkpoint-66
[INFO|configuration_utils.py:697] 2025-02-21 13:58:19,517 >> loading configuration file config.json
[INFO|configuration_utils.py:771] 2025-02-21 13:58:19,518 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.49.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2500] 2025-02-21 13:58:19,671 >> tokenizer config file saved in checkpoints/checkpoint-66/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-02-21 13:58:19,672 >> Special tokens file saved in checkpoints/checkpoint-66/special_tokens_map.json
[INFO|trainer.py:2657] 2025-02-21 13:58:20,222 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 1097.7226, 'train_samples_per_second': 0.246, 'train_steps_per_second': 0.06, 'train_loss': 0.05221469903534109, 'epoch': 2.89}
100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 66/66 [18:17<00:00, 16.63s/it]
[INFO|trainer.py:3942] 2025-02-21 13:58:20,237 >> Saving model checkpoint to checkpoints/
[INFO|configuration_utils.py:697] 2025-02-21 13:58:20,261 >> loading configuration file Qwen/Qwen2.5-1.5B-Instruct/config.json
[INFO|configuration_utils.py:771] 2025-02-21 13:58:20,261 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 1536,
  "initializer_range": 0.02,
  "intermediate_size": 8960,
  "max_position_embeddings": 32768,
  "max_window_layers": 21,
  "model_type": "qwen2",
  "num_attention_heads": 12,
  "num_hidden_layers": 28,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "tie_word_embeddings": true,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.49.0",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 151936
}

[INFO|tokenization_utils_base.py:2500] 2025-02-21 13:58:20,441 >> tokenizer config file saved in checkpoints/tokenizer_config.json
[INFO|tokenization_utils_base.py:2509] 2025-02-21 13:58:20,442 >> Special tokens file saved in checkpoints/special_tokens_map.json
***** train metrics *****
  epoch                    =     2.8889
  total_flos               =  1278715GF
  train_loss               =     0.0522
  train_runtime            = 0:18:17.72
  train_samples_per_second =      0.246
  train_steps_per_second   =       0.06
[INFO|trainer.py:4258] 2025-02-21 13:58:20,605 >>
***** Running Evaluation *****
[INFO|trainer.py:4260] 2025-02-21 13:58:20,605 >>   Num examples = 10
[INFO|trainer.py:4263] 2025-02-21 13:58:20,605 >>   Batch size = 1
100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:17<00:00,  1.73s/it]
***** eval metrics *****
  epoch                   =     2.8889
  eval_loss               =     0.0325
  eval_runtime            = 0:00:19.36
  eval_samples_per_second =      0.516
  eval_steps_per_second   =      0.516
[INFO|modelcard.py:449] 2025-02-21 13:58:39,984 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}